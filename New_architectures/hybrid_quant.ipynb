{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea865d97-f383-4422-b660-7d34e98bb6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloading 50 files to 'image_folder'\n",
      "[SKIP] Already exists: image_folder/backpack.npy\n",
      "[SKIP] Already exists: image_folder/banana.npy\n",
      "[SKIP] Already exists: image_folder/beard.npy\n",
      "[SKIP] Already exists: image_folder/bird.npy\n",
      "[SKIP] Already exists: image_folder/bread.npy\n",
      "[SKIP] Already exists: image_folder/bridge.npy\n",
      "[SKIP] Already exists: image_folder/bush.npy\n",
      "[SKIP] Already exists: image_folder/cactus.npy\n",
      "[SKIP] Already exists: image_folder/candle.npy\n",
      "[SKIP] Already exists: image_folder/camel.npy\n",
      "[SKIP] Already exists: image_folder/bat.npy\n",
      "[SKIP] Already exists: image_folder/butterfly.npy\n",
      "[SKIP] Already exists: image_folder/bicycle.npy\n",
      "[SKIP] Already exists: image_folder/book.npy\n",
      "[SKIP] Already exists: image_folder/bucket.npy\n",
      "[SKIP] Already exists: image_folder/camera.npy\n",
      "[SKIP] Already exists: image_folder/cow.npy\n",
      "[SKIP] Already exists: image_folder/crab.npy\n",
      "[SKIP] Already exists: image_folder/cup.npy\n",
      "[SKIP] Already exists: image_folder/dumbbell.npy\n",
      "[SKIP] Already exists: image_folder/crown.npy\n",
      "[SKIP] Already exists: image_folder/donut.npy\n",
      "[SKIP] Already exists: image_folder/eye.npy\n",
      "[SKIP] Already exists: image_folder/elbow.npy\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/flashlight.npy\n",
      "[SKIP] Already exists: image_folder/fish.npy\n",
      "[SKIP] Already exists: image_folder/flip flops.npy\n",
      "[SKIP] Already exists: image_folder/flower.npy\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/foot.npy\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hat.npy\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hot air balloon.npy\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/helicopter.npy\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/leaf.npy\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/light bulb.npy\n",
      "[SKIP] Already exists: image_folder/lightning.npy\n",
      "[SKIP] Already exists: image_folder/mouth.npy\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/motorbike.npy\n",
      "[SKIP] Already exists: image_folder/leg.npy\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pencil.npy\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/nail.npy\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pillow.npy\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/river.npy\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/school bus.npy\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/sock.npy\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/spoon.npy\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/table.npy\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/telephone.npy\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tooth.npy\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tree.npy\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/umbrella.npy\n",
      "[INFO] Downloading 50 files to 'strokes_data'\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/backpack.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/banana.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/beard.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/bat.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/bicycle.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/bird.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/bread.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/book.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/bridge.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/bucket.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/bush.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/butterfly.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/cactus.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/camel.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/camera.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/candle.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/cow.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/crab.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/crown.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/cup.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/donut.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/dumbbell.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/elbow.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/eye.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/fish.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/flashlight.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/flip flops.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/flower.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/foot.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/hat.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/helicopter.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/hot air balloon.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/leaf.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/leg.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/light bulb.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/lightning.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/motorbike.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/mouth.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/nail.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/pencil.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/pillow.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/river.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/school bus.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/sock.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/spoon.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/table.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/telephone.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/tooth.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/tree.npz\n",
      "[DOWNLOAD] https://storage.googleapis.com/quickdraw_dataset/sketchrnn/umbrella.npz\n"
     ]
    }
   ],
   "source": [
    "# @title Download .npy & .npz Dataset\n",
    "\n",
    "import concurrent.futures\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import requests\n",
    "\n",
    "# Shared class names (without extension, to be extended based on type)\n",
    "base_classes = [\n",
    "    \"backpack\",\n",
    "    \"banana\",\n",
    "    \"bat\",\n",
    "    \"beard\",\n",
    "    \"bicycle\",\n",
    "    \"bird\",\n",
    "    \"book\",\n",
    "    \"bread\",\n",
    "    \"bridge\",\n",
    "    \"bucket\",\n",
    "    \"bush\",\n",
    "    \"butterfly\",\n",
    "    \"cactus\",\n",
    "    \"camel\",\n",
    "    \"camera\",\n",
    "    \"candle\",\n",
    "    \"cow\",\n",
    "    \"crab\",\n",
    "    \"crown\",\n",
    "    \"cup\",\n",
    "    \"donut\",\n",
    "    \"dumbbell\",\n",
    "    \"elbow\",\n",
    "    \"eye\",\n",
    "    \"fish\",\n",
    "    \"flashlight\",\n",
    "    \"flip flops\",\n",
    "    \"flower\",\n",
    "    \"foot\",\n",
    "    \"hat\",\n",
    "    \"helicopter\",\n",
    "    \"hot air balloon\",\n",
    "    \"leaf\",\n",
    "    \"leg\",\n",
    "    \"light bulb\",\n",
    "    \"lightning\",\n",
    "    \"motorbike\",\n",
    "    \"mouth\",\n",
    "    \"nail\",\n",
    "    \"pencil\",\n",
    "    \"pillow\",\n",
    "    \"river\",\n",
    "    \"school bus\",\n",
    "    \"sock\",\n",
    "    \"spoon\",\n",
    "    \"table\",\n",
    "    \"telephone\",\n",
    "    \"tooth\",\n",
    "    \"tree\",\n",
    "    \"umbrella\",\n",
    "]\n",
    "\n",
    "\n",
    "def fetch_xml(xml_url):\n",
    "    response = requests.get(xml_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"[ERROR] Failed to fetch XML: {response.status_code}\")\n",
    "        return None\n",
    "    return response.content\n",
    "\n",
    "\n",
    "def parse_xml(xml_content, selected_files, prefix_filter):\n",
    "    root = ET.fromstring(xml_content)\n",
    "    namespace = {\"s3\": \"http://doc.s3.amazonaws.com/2006-03-01\"}\n",
    "    base_url = \"https://storage.googleapis.com/quickdraw_dataset/\"\n",
    "    file_urls = []\n",
    "\n",
    "    for content in root.findall(\".//s3:Contents\", namespace):\n",
    "        key = content.find(\"s3:Key\", namespace).text\n",
    "        file_name = os.path.basename(key)\n",
    "\n",
    "        if prefix_filter and not key.startswith(prefix_filter):\n",
    "            continue\n",
    "        if file_name in selected_files:\n",
    "            file_urls.append(base_url + key)\n",
    "    return file_urls\n",
    "\n",
    "\n",
    "def download_file(file_url, download_folder):\n",
    "    file_path = os.path.join(download_folder, os.path.basename(file_url))\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"[SKIP] Already exists: {file_path}\")\n",
    "        return\n",
    "    print(f\"[DOWNLOAD] {file_url}\")\n",
    "    response = requests.get(file_url)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(\n",
    "            f\"[FAIL] Could not download: {file_url} - Status code: {response.status_code}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def download_quickdraw_files(\n",
    "    xml_url, download_folder, file_type=\"npy\", prefix_filter=\"\"\n",
    "):\n",
    "    selected_files = [name + f\".{file_type}\" for name in base_classes]\n",
    "\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "\n",
    "    xml_content = fetch_xml(xml_url)\n",
    "    if xml_content is None:\n",
    "        return\n",
    "\n",
    "    file_urls = parse_xml(xml_content, selected_files, prefix_filter)\n",
    "\n",
    "    print(f\"[INFO] Downloading {len(file_urls)} files to '{download_folder}'\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        executor.map(lambda url: download_file(url, download_folder), file_urls)\n",
    "\n",
    "\n",
    "# Download image .npy files\n",
    "download_quickdraw_files(\n",
    "    xml_url=\"https://storage.googleapis.com/quickdraw_dataset/\",\n",
    "    download_folder=\"image_folder\",\n",
    "    file_type=\"npy\",\n",
    "    prefix_filter=\"\",\n",
    ")\n",
    "\n",
    "# Download stroke .npz files\n",
    "download_quickdraw_files(\n",
    "    xml_url=\"https://storage.googleapis.com/quickdraw_dataset?prefix=sketchrnn/\",\n",
    "    download_folder=\"strokes_data\",\n",
    "    file_type=\"npz\",\n",
    "    prefix_filter=\"sketchrnn/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b36d7c0-aec5-4078-8385-3891eb927f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import necesary\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    LSTM,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Concatenate,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import (\n",
    "    Bidirectional,\n",
    "    Input,\n",
    "    LSTM,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Concatenate,\n",
    ")\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1819de62-5148-404c-ba1d-03ca5f7c8244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Settings\n",
    "\n",
    "MAX_SEQ_LEN = 130\n",
    "STROKE_FEATURES = 3\n",
    "IMG_HEIGHT, IMG_WIDTH = 28, 28\n",
    "IMG_CHANNELS = 1\n",
    "NUM_CLASSES = 50\n",
    "SAMPLES_PER_CLASS = 5000\n",
    "DATA_DIR_STROKES = \"strokes_data\"\n",
    "DATA_DIR_IMAGES = \"image_folder\"\n",
    "VALIDATION_SPLIT = 0.1\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d697b22-e791-4c75-9a81-8694aaf0dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_stroke(stroke, max_len=MAX_SEQ_LEN):\n",
    "    \"\"\"\n",
    "    Improved stroke preprocessing with consistent normalization\n",
    "    Centers to (0,0) and scales to [-100, 100] range\n",
    "    \"\"\"\n",
    "    stroke = stroke.astype(np.float32)\n",
    "    \n",
    "    # Convert to absolute coordinates\n",
    "    stroke[:, 0] = np.cumsum(stroke[:, 0])\n",
    "    stroke[:, 1] = np.cumsum(stroke[:, 1])\n",
    "    \n",
    "    # Center to (0, 0)\n",
    "    stroke[:, 0] -= stroke[:, 0].mean()\n",
    "    stroke[:, 1] -= stroke[:, 1].mean()\n",
    "    \n",
    "    # Scale to [-100, 100] range\n",
    "    if len(stroke) > 0:\n",
    "        # Find the maximum absolute coordinate value\n",
    "        max_coord = max(\n",
    "            np.abs(stroke[:, 0]).max() if len(stroke) > 0 else 1,\n",
    "            np.abs(stroke[:, 1]).max() if len(stroke) > 0 else 1\n",
    "        )\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if max_coord > 0:\n",
    "            # Scale to [-100, 100] range\n",
    "            scale_factor = 100.0 / max_coord\n",
    "            stroke[:, 0] *= scale_factor\n",
    "            stroke[:, 1] *= scale_factor\n",
    "    \n",
    "    # Truncate or pad as before\n",
    "    if len(stroke) > max_len:\n",
    "        return stroke[:max_len]\n",
    "    \n",
    "    pad = np.zeros((max_len - len(stroke), STROKE_FEATURES), dtype=np.float32)\n",
    "    return np.vstack([stroke, pad])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3402e38-06de-4950-a2d7-9cfe83e8f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hybrid_data(N=SAMPLES_PER_CLASS):\n",
    "    img_files_full = sorted(\n",
    "        f for f in os.listdir(DATA_DIR_IMAGES) if f.endswith(\".npy\")\n",
    "    )\n",
    "    stroke_files_full = sorted(\n",
    "        f for f in os.listdir(DATA_DIR_STROKES) if f.endswith(\".npz\")\n",
    "    )\n",
    "    \n",
    "    img_names = {os.path.splitext(f)[0] for f in img_files_full}\n",
    "    stroke_names = {os.path.splitext(f)[0] for f in stroke_files_full}\n",
    "    common = sorted(img_names & stroke_names)[:NUM_CLASSES]\n",
    "\n",
    "    X_img_list, X_str_list, y_list = [], [], []\n",
    "    for idx, cls in enumerate(common):\n",
    "        img_arr = np.load(\n",
    "            os.path.join(DATA_DIR_IMAGES, f\"{cls}.npy\"),\n",
    "            allow_pickle=True,\n",
    "            encoding=\"latin1\",\n",
    "        )[:N]\n",
    "        img_arr = (\n",
    "            img_arr.reshape(-1, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS).astype(\"float32\")\n",
    "            / 255.0\n",
    "        )\n",
    "        X_img_list.append(img_arr)\n",
    "\n",
    "        data = np.load(\n",
    "            os.path.join(DATA_DIR_STROKES, f\"{cls}.npz\"),\n",
    "            allow_pickle=True,\n",
    "            encoding=\"latin1\",\n",
    "        )\n",
    "        strokes = data[\"train\"][:N]\n",
    "        proc = np.stack([preprocess_stroke(s) for s in strokes], axis=0)\n",
    "        X_str_list.append(proc)\n",
    "\n",
    "        y_list.append(\n",
    "            np.full((N,), idx, dtype=np.int32)\n",
    "        )  #  putting labels same for each class\n",
    "\n",
    "    X_img = np.concatenate(X_img_list, axis=0)\n",
    "    X_str = np.concatenate(X_str_list, axis=0)\n",
    "    y = np.concatenate(y_list, axis=0)\n",
    "    X_img, X_str, y = shuffle(\n",
    "        X_img, X_str, y, random_state=42\n",
    "    )  #  mix everything randomly\n",
    "\n",
    "    true_num_classes = len(common)\n",
    "    y_cat = to_categorical(y, num_classes=true_num_classes)\n",
    "    return (X_str, X_img), y_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc7877b8-6f57-4c43-b210-b7faceccfea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hybrid_model():\n",
    "    # Stroke input branch: captures temporal dependency of pen stokes using  Bidirectional LSTMs\n",
    "    inp_str = Input(shape=(MAX_SEQ_LEN, STROKE_FEATURES), name=\"stroke_input\")\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(inp_str)\n",
    "    x = Bidirectional(LSTM(64))(x)\n",
    "    x = Dense(64, activation=\"relu\")(\n",
    "        x\n",
    "    )  # Dense layer to compact learned stroke features\n",
    "\n",
    "    # Image input branch: processes sketch image using a CNN to extract spatial patterns\n",
    "    inp_img = Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), name=\"image_input\")\n",
    "    y = Conv2D(32, 3, activation=\"relu\", padding=\"same\")(inp_img)\n",
    "    y = MaxPooling2D()(y)\n",
    "    y = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(y)\n",
    "    y = MaxPooling2D()(y)\n",
    "    y = Conv2D(128, 3, activation=\"relu\", padding=\"same\")(y)\n",
    "    y = MaxPooling2D()(y)\n",
    "    y = Flatten()(y)\n",
    "    y = Dense(128, activation=\"relu\")(y)\n",
    "\n",
    "    # Feature fusion: concatenate outputs from stroke and image branches\n",
    "    merged = Concatenate()([x, y])\n",
    "    merged = Dropout(0.5)(merged)  # dropout\n",
    "    merged = Dense(128, activation=\"relu\")(merged)\n",
    "    out = Dense(NUM_CLASSES, activation=\"softmax\")(merged)\n",
    "\n",
    "    return Model(inputs=[inp_str, inp_img], outputs=out, name=\"hybrid_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89190697-60d3-4a42-99e9-6121307cdc77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bbc0396-1d93-4323-b261-201cc1d8170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_str, X_img), y = load_hybrid_data(N=SAMPLES_PER_CLASS)\n",
    "NUM_CLASSES = y.shape[1]\n",
    "total = X_img.shape[0]\n",
    "split = int((1 - VALIDATION_SPLIT) * total)\n",
    "\n",
    "# trian test split\n",
    "X_str_train, X_str_val = X_str[:split], X_str[split:]\n",
    "X_img_train, X_img_val = X_img[:split], X_img[split:]\n",
    "y_train, y_val = y[:split], y[split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a3f0694-020b-43aa-af44-7a44e5a91ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"/Users/aryanagarwal/Desktop/cv/project/main_paper/Doodle-vision/inference/models/best_hybrid_model_strokes_scaled.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1336ab71-ddc8-49bf-82d5-94d4f696d727",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"hybrid_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"hybrid_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ image_input         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ stroke_input        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">135,168</span> │ stroke_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,450</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ image_input         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    │        \u001b[38;5;34m320\u001b[0m │ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ stroke_input        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m73,856\u001b[0m │ max_pooling2d_1[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m135,168\u001b[0m │ stroke_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m164,352\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1152\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_2[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ bidirectional_1[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m147,584\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m24,704\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │      \u001b[38;5;34m6,450\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,737,560</span> (6.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,737,560\u001b[0m (6.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">579,186</span> (2.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m579,186\u001b[0m (2.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,158,374</span> (4.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,158,374\u001b[0m (4.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d59b181e-2399-4fa2-9801-46620bc10a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05378928358074953"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(24704+6450)/579186"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91420884-0a3a-46e7-b135-147455f1f346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31cd6b95-173a-404e-9864-c25db34c74c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "feature_model = Model(inputs=model.input, outputs=model.get_layer(\"concatenate\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1b0b947-5ff9-44d2-aa55-4833d21ea44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1758/1758\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 100ms/step\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 103ms/step\n",
      "Random Forest Validation Accuracy: 0.9366\n"
     ]
    }
   ],
   "source": [
    "X_train_feats = feature_model.predict([X_str_train, X_img_train], batch_size=BATCH_SIZE)\n",
    "X_val_feats = feature_model.predict([X_str_val, X_img_val], batch_size=BATCH_SIZE)\n",
    "\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "y_val_labels = np.argmax(y_val, axis=1)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "475043f0-c04f-4d2f-bb8c-80a1d253ab00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Validation Accuracy: 0.7964\n",
      "Random Forest Training Accuracy: 0.8232533333333333\n",
      "Model size: 0.86 MB\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=30,max_depth=5, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train_feats, y_train_labels)\n",
    "y_pred = rf.predict(X_val_feats)\n",
    "print(\"Random Forest Validation Accuracy:\", accuracy_score(y_val_labels, y_pred))\n",
    "\n",
    "y_pred_train = rf.predict(X_train_feats)\n",
    "print(\"Random Forest Training Accuracy:\", accuracy_score(y_train_labels, y_pred_train))\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "joblib.dump(rf, \"rf_model.pkl\")\n",
    "print(f\"Model size: {os.path.getsize('rf_model.pkl') / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6de4eaa8-cc4e-4392-a3fd-96f05003b032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2991           12.50m\n",
      "         2           1.1455            6.26m\n",
      "         3           1.0171            0.00s\n",
      "XGBoost Validation Accuracy: 0.83344\n",
      "XGBoost Training Accuracy: 0.8822\n",
      "Model size: 0.91 MB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=3,       # reduce number of boosting rounds\n",
    "    learning_rate=0.1,     # step size\n",
    "    max_depth=5,           # depth of each tree\n",
    "    # subsample=0.8,         # stochastic gradient boosting\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "gb_model.fit(X_train_feats, y_train_labels)\n",
    "y_pred = gb_model.predict(X_val_feats)\n",
    "print(\"XGBoost Validation Accuracy:\", accuracy_score(y_val_labels, y_pred))\n",
    "\n",
    "y_pred_train = gb_model.predict(X_train_feats)\n",
    "print(\"XGBoost Training Accuracy:\", accuracy_score(y_train_labels, y_pred_train))\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "joblib.dump(gb_model, \"gb_model.pkl\")\n",
    "print(f\"Model size: {os.path.getsize('gb_model.pkl') / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a155c59a-29ad-480f-8c43-efbd9fffeb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09290b5c-877d-4fef-8acb-efcc72441edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train cf\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       backpack       1.00      1.00      1.00      4476\n",
      "         banana       1.00      1.00      1.00      4484\n",
      "            bat       1.00      1.00      1.00      4520\n",
      "          beard       1.00      1.00      1.00      4457\n",
      "        bicycle       1.00      1.00      1.00      4474\n",
      "           bird       1.00      1.00      1.00      4496\n",
      "           book       1.00      1.00      1.00      4521\n",
      "          bread       1.00      1.00      1.00      4492\n",
      "         bridge       1.00      1.00      1.00      4509\n",
      "         bucket       1.00      1.00      1.00      4505\n",
      "           bush       1.00      1.00      1.00      4524\n",
      "      butterfly       1.00      1.00      1.00      4491\n",
      "         cactus       1.00      1.00      1.00      4543\n",
      "          camel       1.00      1.00      1.00      4482\n",
      "         camera       1.00      1.00      1.00      4532\n",
      "         candle       1.00      1.00      1.00      4497\n",
      "            cow       1.00      1.00      1.00      4510\n",
      "           crab       1.00      1.00      1.00      4459\n",
      "          crown       1.00      1.00      1.00      4492\n",
      "            cup       1.00      1.00      1.00      4529\n",
      "          donut       1.00      1.00      1.00      4490\n",
      "       dumbbell       1.00      1.00      1.00      4530\n",
      "          elbow       1.00      1.00      1.00      4475\n",
      "            eye       1.00      1.00      1.00      4522\n",
      "           fish       1.00      1.00      1.00      4498\n",
      "     flashlight       1.00      1.00      1.00      4504\n",
      "     flip flops       1.00      1.00      1.00      4480\n",
      "         flower       1.00      1.00      1.00      4465\n",
      "           foot       1.00      1.00      1.00      4493\n",
      "            hat       1.00      1.00      1.00      4490\n",
      "     helicopter       1.00      1.00      1.00      4479\n",
      "hot air balloon       1.00      1.00      1.00      4455\n",
      "           leaf       1.00      1.00      1.00      4529\n",
      "            leg       1.00      1.00      1.00      4505\n",
      "     light bulb       1.00      1.00      1.00      4544\n",
      "      lightning       1.00      1.00      1.00      4495\n",
      "      motorbike       1.00      1.00      1.00      4523\n",
      "          mouth       1.00      1.00      1.00      4510\n",
      "           nail       1.00      1.00      1.00      4476\n",
      "         pencil       1.00      1.00      1.00      4553\n",
      "         pillow       1.00      1.00      1.00      4510\n",
      "          river       1.00      1.00      1.00      4480\n",
      "     school bus       1.00      1.00      1.00      4494\n",
      "           sock       1.00      1.00      1.00      4479\n",
      "          spoon       1.00      1.00      1.00      4507\n",
      "          table       1.00      1.00      1.00      4516\n",
      "      telephone       1.00      1.00      1.00      4493\n",
      "          tooth       1.00      1.00      1.00      4498\n",
      "           tree       1.00      1.00      1.00      4543\n",
      "       umbrella       1.00      1.00      1.00      4471\n",
      "\n",
      "       accuracy                           1.00    225000\n",
      "      macro avg       1.00      1.00      1.00    225000\n",
      "   weighted avg       1.00      1.00      1.00    225000\n",
      "\n",
      "validation cf\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       backpack       0.93      0.95      0.94       524\n",
      "         banana       0.96      0.96      0.96       516\n",
      "            bat       0.90      0.87      0.89       480\n",
      "          beard       0.98      0.94      0.96       543\n",
      "        bicycle       0.83      0.83      0.83       526\n",
      "           bird       0.83      0.85      0.84       504\n",
      "           book       0.93      0.97      0.95       479\n",
      "          bread       0.89      0.90      0.89       508\n",
      "         bridge       0.91      0.94      0.92       491\n",
      "         bucket       0.98      0.97      0.97       495\n",
      "           bush       0.93      0.93      0.93       476\n",
      "      butterfly       0.97      0.98      0.98       509\n",
      "         cactus       0.96      0.98      0.97       457\n",
      "          camel       0.95      0.96      0.96       518\n",
      "         camera       0.98      0.99      0.99       468\n",
      "         candle       0.97      0.99      0.98       503\n",
      "            cow       0.94      0.96      0.95       490\n",
      "           crab       0.93      0.94      0.94       541\n",
      "          crown       0.98      0.98      0.98       508\n",
      "            cup       0.92      0.92      0.92       471\n",
      "          donut       0.99      0.99      0.99       510\n",
      "       dumbbell       0.97      0.94      0.95       470\n",
      "          elbow       0.89      0.91      0.90       525\n",
      "            eye       0.96      0.97      0.96       478\n",
      "           fish       0.98      0.95      0.96       502\n",
      "     flashlight       0.89      0.93      0.91       496\n",
      "     flip flops       0.95      0.93      0.94       520\n",
      "         flower       0.98      0.97      0.97       535\n",
      "           foot       0.91      0.90      0.90       507\n",
      "            hat       0.96      0.95      0.95       510\n",
      "     helicopter       0.93      0.97      0.95       521\n",
      "hot air balloon       0.93      0.90      0.91       545\n",
      "           leaf       0.94      0.94      0.94       471\n",
      "            leg       0.92      0.92      0.92       495\n",
      "     light bulb       0.85      0.88      0.87       456\n",
      "      lightning       0.95      0.93      0.94       505\n",
      "      motorbike       0.79      0.81      0.80       477\n",
      "          mouth       0.96      0.97      0.96       490\n",
      "           nail       0.95      0.87      0.91       524\n",
      "         pencil       0.93      0.96      0.94       447\n",
      "         pillow       0.95      0.93      0.94       490\n",
      "          river       0.95      0.93      0.94       520\n",
      "     school bus       0.98      0.98      0.98       506\n",
      "           sock       0.96      0.94      0.95       521\n",
      "          spoon       0.95      0.93      0.94       493\n",
      "          table       0.97      0.97      0.97       484\n",
      "      telephone       0.90      0.92      0.91       507\n",
      "          tooth       0.98      0.93      0.95       502\n",
      "           tree       0.96      0.96      0.96       457\n",
      "       umbrella       0.98      0.97      0.98       529\n",
      "\n",
      "       accuracy                           0.94     25000\n",
      "      macro avg       0.94      0.94      0.94     25000\n",
      "   weighted avg       0.94      0.94      0.94     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('train cf')\n",
    "print(classification_report(y_train_labels, y_pred_train, target_names=base_classes))\n",
    "\n",
    "\n",
    "print('validation cf')\n",
    "print(classification_report(y_val_labels, y_pred, target_names=base_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc8799bf-3b42-41f6-8d22-fb64b2fed201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying knowledge distillation and quantisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84664d9-360e-4e20-b5cf-d294a76530a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow_model_optimization.quantization.keras import vitis_quantize\n",
    "\n",
    "# Build a smaller student model\n",
    "def build_student_model():\n",
    "    inp_str = keras.Input(shape=(130, 3), name=\"stroke_input\")\n",
    "    x = keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True))(inp_str)\n",
    "    x = keras.layers.Bidirectional(keras.layers.LSTM(32))(x)\n",
    "    x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "\n",
    "    inp_img = keras.Input(shape=(28, 28, 1), name=\"image_input\")\n",
    "    y = keras.layers.Conv2D(16, 3, activation=\"relu\", padding=\"same\")(inp_img)\n",
    "    y = keras.layers.MaxPooling2D()(y)\n",
    "    y = keras.layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(y)\n",
    "    y = keras.layers.MaxPooling2D()(y)\n",
    "    y = keras.layers.Flatten()(y)\n",
    "    y = keras.layers.Dense(64, activation=\"relu\")(y)\n",
    "\n",
    "    merged = keras.layers.Concatenate()([x, y])\n",
    "    merged = keras.layers.Dropout(0.3)(merged)\n",
    "    merged = keras.layers.Dense(64, activation=\"relu\")(merged)\n",
    "    out = keras.layers.Dense(NUM_CLASSES, activation=\"softmax\")(merged)\n",
    "\n",
    "    return keras.Model(inputs=[inp_str, inp_img], outputs=out)\n",
    "\n",
    "student_model = build_student_model()\n",
    "\n",
    "# Distillation class\n",
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super().__init__()\n",
    "        self.student = student\n",
    "        self.teacher = teacher\n",
    "        self.teacher.trainable = False\n",
    "\n",
    "    def compile(self, optimizer, metrics, student_loss_fn, distill_loss_fn, alpha=0.5, temperature=5):\n",
    "        super().compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distill_loss_fn = distill_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        teacher_preds = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            student_preds = self.student(x, training=True)\n",
    "            student_loss = self.student_loss_fn(y, student_preds)\n",
    "            distill_loss = self.distill_loss_fn(\n",
    "                tf.nn.softmax(teacher_preds / self.temperature, axis=1),\n",
    "                tf.nn.softmax(student_preds / self.temperature, axis=1),\n",
    "            )\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distill_loss\n",
    "\n",
    "        grads = tape.gradient(loss, self.student.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.student.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, student_preds)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "# Compile distiller\n",
    "distiller = Distiller(student=student_model, teacher=teacher_model)\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    "    student_loss_fn=keras.losses.CategoricalCrossentropy(),\n",
    "    distill_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.5,\n",
    "    temperature=5,\n",
    ")\n",
    "\n",
    "# Train\n",
    "distiller.fit(\n",
    "    [X_str_train, X_img_train],\n",
    "    y_train,\n",
    "    validation_data=([X_str_val, X_img_val], y_val),\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    ")\n",
    "\n",
    "# Quantization (optional)\n",
    "quantizer = vitis_quantize.VitisQuantizer(student_model)\n",
    "quantized_model = quantizer.quantize_model(calib_dataset=([X_str_train[:200], X_img_train[:200]]))\n",
    "\n",
    "# Save quantized model\n",
    "quantized_model.save(\"quantized_student_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f02f6772-35f5-4057-9be3-b431c2954cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-model-optimization in /opt/anaconda3/lib/python3.11/site-packages (0.8.0)\n",
      "Requirement already satisfied: absl-py~=1.2 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.4.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-model-optimization) (0.1.9)\n",
      "Requirement already satisfied: numpy~=1.23 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.26.4)\n",
      "Requirement already satisfied: six~=1.14 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.16.0)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from dm-tree~=0.1.1->tensorflow-model-optimization) (24.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in /opt/anaconda3/lib/python3.11/site-packages (from dm-tree~=0.1.1->tensorflow-model-optimization) (1.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44ac1d77-1b48-4851-8454-46d4e7ba5d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/gh/1nq7ydsd0ybb433gq3p_mk5c0000gn/T/tmpgw8a17rt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/gh/1nq7ydsd0ybb433gq3p_mk5c0000gn/T/tmpgw8a17rt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/gh/1nq7ydsd0ybb433gq3p_mk5c0000gn/T/tmpgw8a17rt'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 130, 3), dtype=tf.float32, name='stroke_input'), TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='image_input')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 50), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  14250378064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250378640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250379792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250379024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250378832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250377872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250382096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250381520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250384016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250381136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250381328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250383248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250385168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250382288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250386704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250384400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250382864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250385552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250387088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250385744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250387664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250386320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250384976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250386512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250388816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14250385936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Saved TFLite model with Select TF Ops support.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1749715053.365126  124644 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1749715053.365135  124644 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-06-12 13:27:33.573073: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3993] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack\n",
      "Details:\n",
      "\ttf.TensorListReserve(tensor<2xi32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<?x128xf32>>>) : {device = \"\"}\n",
      "\ttf.TensorListReserve(tensor<2xi32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<?x64xf32>>>) : {device = \"\"}\n",
      "\ttf.TensorListSetItem(tensor<!tf_type.variant<tensor<?x128xf32>>>, tensor<i32>, tensor<?x128xf32>) -> (tensor<!tf_type.variant<tensor<?x128xf32>>>) : {device = \"\", resize_if_index_out_of_bounds = false}\n",
      "\ttf.TensorListSetItem(tensor<!tf_type.variant<tensor<?x64xf32>>>, tensor<i32>, tensor<?x64xf32>) -> (tensor<!tf_type.variant<tensor<?x64xf32>>>) : {device = \"\", resize_if_index_out_of_bounds = false}\n",
      "\ttf.TensorListStack(tensor<!tf_type.variant<tensor<?x128xf32>>>, tensor<2xi32>) -> (tensor<130x?x128xf32>) : {device = \"\", num_elements = 130 : i64}\n",
      "\ttf.TensorListStack(tensor<!tf_type.variant<tensor<?x64xf32>>>, tensor<2xi32>) -> (tensor<1x?x64xf32>) : {device = \"\", num_elements = 1 : i64}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n"
     ]
    }
   ],
   "source": [
    "# Create converter\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# Allow unsupported ops (like TensorList ops used by LSTM)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS,\n",
    "]\n",
    "\n",
    "# Prevent lowering of TensorList ops\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "# Optimize (optional quantization)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Convert\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save\n",
    "with open(\"student_model_lstm_compatible.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Saved TFLite model with Select TF Ops support.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f00c3b81-d37e-410b-9a4a-688bf9e6b4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 0.64 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"student_model_lstm_compatible.tflite\"\n",
    "file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "print(f\"Model size: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "645fdeb1-3b7d-4754-9634-2dc0456a88ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input details: [{'name': 'serving_default_image_input:0', 'index': 0, 'shape': array([ 1, 28, 28,  1], dtype=int32), 'shape_signature': array([-1, 28, 28,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'serving_default_stroke_input:0', 'index': 1, 'shape': array([  1, 130,   3], dtype=int32), 'shape_signature': array([ -1, 130,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Output details: [{'name': 'StatefulPartitionedCall_1:0', 'index': 92, 'shape': array([ 1, 50], dtype=int32), 'shape_signature': array([-1, 50], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
      "INFO: Created TensorFlow Lite delegate for select TF ops.\n",
      "INFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 42 nodes with 3 partitions.\n",
      "\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=\"student_model_lstm_compatible.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"Input details:\", input_details)\n",
    "print(\"Output details:\", output_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf00391-8b67-4c79-bbcc-21fda5ea9388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trian test split\n",
    "X_str_train, X_str_val = X_str[:split], X_str[split:]\n",
    "X_img_train, X_img_val = X_img[:split], X_img[split:]\n",
    "y_train, y_val = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "985396d5-6496-4027-a2a7-ea4685d4bf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input details:\n",
      "Input 0: name = serving_default_image_input:0, shape = [ 1 28 28  1], dtype = <class 'numpy.float32'>\n",
      "Input 1: name = serving_default_stroke_input:0, shape = [  1 130   3], dtype = <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input details:\")\n",
    "for i, d in enumerate(input_details):\n",
    "    print(f\"Input {i}: name = {d['name']}, shape = {d['shape']}, dtype = {d['dtype']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c11849a7-28d2-4dba-bd45-db8a67103465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite model accuracy: 94.02%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = len(X_str_val)\n",
    "\n",
    "for i in range(total):\n",
    "    input_stroke = X_str_val[i:i+1].astype(np.float32)  # shape [1, 130, 3]\n",
    "    input_image = X_img_val[i:i+1].astype(np.float32)   # shape [1, 28, 28, 1]\n",
    "\n",
    "    # Set inputs (order might differ — check your model)\n",
    "    interpreter.set_tensor(input_details[1]['index'], input_stroke)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    pred_label = np.argmax(output)\n",
    "    true_label = np.argmax(y_val[i])\n",
    "\n",
    "    if pred_label == true_label:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"TFLite model accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f42d8-1cda-4a80-96c0-15b5dc37802c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
